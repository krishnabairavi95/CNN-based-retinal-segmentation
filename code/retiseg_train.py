
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import SimpleITK as sitk
import numpy as np
import os
import numpy as np
import tensorflow as tf
from keras import backend as K
from keras.models import *
from numpy import load
from keras.layers import *
from keras.optimizers import *
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras import backend as keras
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
import glob

### Pleaae change the model name to Unet/SegNet in line 232 while training.

data = load('data_train_final.npz')    ### Preprocessed data from preprocess.py
mask_set=data["mask_set"]
ground_truth_list=data["ground_truth_list"]
train_list=data["train_list"]


## Input image is split into small 48x48 patches. Patches generated by choosing centers randomly

def get_train(train_img, mask_img,train_groudTruth, patch_h, patch_w, N_subimgs,inside=True):
    for j in range(len(train_img)):
        train_array=train_img[j][9:574,:]
        train_norm= train_array/np.max(train_array)
        mask_array=mask_img[j][9:574,:]
        mask_norm= mask_array/np.max(mask_array)
        ground_array=train_groudTruth[j][9:574,:]
        ground_norm= ground_array/np.max(ground_array)
        train_combined,patches_imgs_train, patches_masks_train,groundtruth_patch  = random_patches(train_norm,mask_norm,ground_norm,patch_h,patch_w,N_subimgs,inside=True)
        if j == 0:
            patches_img = patches_imgs_train
            patches_mask = patches_masks_train
            ground_patch= groundtruth_patch
        else:
            patches_img = np.vstack((patches_img, patches_imgs_train))
            patches_mask = np.vstack((patches_mask, patches_masks_train))
            ground_patch= np.vstack((ground_patch,groundtruth_patch  ))
        
        return train_combined, patches_img, patches_mask, groundtruth_patch


## Sub function to generate random patches
def random_patches(train_imgs,train_masks,train_groudTruth,patch_h,patch_w,N_patches,inside=True):
    print (train_imgs.shape)
    img_h = train_imgs.shape[0]
    img_w = train_imgs.shape[1]
    patch_per_img = int(N_patches/20)
    patches = np.empty((N_patches,patch_h,patch_w))
    patches_masks= np.empty((N_patches,patch_h,patch_w))
    patches_ground= np.empty((N_patches,patch_h,patch_w,1))
    
    k=0
    train_combined=np.zeros((N_patches,patch_h,patch_w,1))
    while(True):
        
        if(k > patch_per_img):
            break
    
        x_center = np.random.randint(0+int(patch_w/2),img_w-int(patch_w/2))
        y_center = np.random.randint(0+int(patch_h/2),img_h-int(patch_h/2))
        
        if( (inside == True and (is_patch_valid(x_center,y_center,img_w,img_h,patch_h) ) ==False ) == False): #To check
            y_start = y_center-int(patch_h/2)
            y_end = y_center+int(patch_h/2)
            x_start = x_center-int(patch_w/2)
            x_end = x_center+int(patch_w/2)
            patch = train_imgs[y_start:y_end,x_start:x_end]
            patch_mask= train_masks[y_start:y_end,x_start:x_end]
            
            ground_patch = train_groudTruth[y_start:y_end,x_start:x_end]
            
            patches[k,:,:]= patch
            patches_masks[k,:,:]= patch_mask
            patches_ground[k,:,:,0]= ground_patch
            k = k+1

    train_combined[:,:,:,0]=patches
    
    return train_combined,patches,patches_masks, patches_ground
    

## To check if rhe generated patch lies within the area of interest:
def is_patch_valid(x,y,img_w,img_h,patch_h):
    
    x_modified = x - int(img_w/2)
    y_modified = y - int(img_h/2)
    R_inside = 270 - int(patch_h * np.sqrt(2.0) / 2.0)
    
    radius = np.sqrt( np.power(x_modified,2) + np.power(y_modified,2) )
    is_inside = True
    
    if radius > R_inside:
        is_inside = False
    
    return is_inside


train_combined,patches,patch_masks,groundtruth_patch= get_train(train_list,mask_set,ground_truth_list,48,48,190000,inside=True)

print (train_combined.shape)
print (groundtruth_patch.shape)


# UNET/SegNET ARCHITECTURE
## Network input- 48X48 patch which goes through a series of encoding and decoding steps and the predicted image is of dimension 48x48

def conv2d_block(input_tensor,n_filters,kernel_size,batchnorm=True):
    # first layer
    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer="he_normal",
               padding="same")(input_tensor)
    if batchnorm:
        x = BatchNormalization()(x)
    x = Activation("relu")(x)
    # second layer
    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), kernel_initializer="he_normal",
               padding="same")(x)
    if batchnorm:
        x = BatchNormalization()(x)
    x = Activation("relu")(x)
    return x


def get_segnet(input_img, n_filters=16, dropout=0.5, batchnorm=True):

    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)
    p1 = MaxPooling2D((2,2)) (c1)
    p1 = Dropout(dropout*0.5)(p1)
    
    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)
    p2 = MaxPooling2D((2,2)) (c2)
    p2 = Dropout(dropout)(p2)
    
    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2,2), padding='same') (p2)
    u8 = Dropout(dropout)(u8)
    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)
    
    u9 = Conv2DTranspose(n_filters*1, (3,3), strides=(2,2), padding='same') (c8)
    u9 = Dropout(dropout)(u9)
    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)
    
    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)
    model = Model(inputs=[input_img], outputs=[outputs])
    return model


def get_unet(input_img, n_filters=16, dropout=0.5, batchnorm=True):
    # contracting path
    c1 = conv2d_block(input_img, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)
    p1 = MaxPooling2D((2,2)) (c1)
    p1 = Dropout(dropout*0.5)(p1)

    c2 = conv2d_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)
    p2 = MaxPooling2D((2,2)) (c2)
    p2 = Dropout(dropout)(p2)

    c3 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)
    p3 = MaxPooling2D((2, 2)) (c3)
    p3 = Dropout(dropout)(p3)

    c4 = conv2d_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)
    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)
    p4 = Dropout(dropout)(p4)

    c5 = conv2d_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)

    # expansive path
    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)
    u6 = concatenate([u6, c4])
    u6 = Dropout(dropout)(u6)
    c6 = conv2d_block(u6, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)

    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)
    u7 = concatenate([u7, c3])
    u7 = Dropout(dropout)(u7)
    c7 = conv2d_block(u7, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)

    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2,2), padding='same') (c5)
    u8 = concatenate([u8, c2])
    u8 = Dropout(dropout)(u8)
    c8 = conv2d_block(u8, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)

    u9 = Conv2DTranspose(n_filters*1, (3,3), strides=(2,2), padding='same') (c8)
    u9 = concatenate([u9, c1], axis=3)
    u9 = Dropout(dropout)(u9)
    c9 = conv2d_block(u9, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)

    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)
    model = Model(inputs=[input_img], outputs=[outputs])
    return model


# Initialising image height and width
im_width = 48
im_height = 48


def data_split(matrix, target, test_proportion):
    ratio = int(matrix.shape[0]/test_proportion)
    X_train = matrix[ratio:,:,:]
    X_test =  matrix[:ratio,:,:]
    Y_train = target[ratio:,:,:]
    Y_test =  target[:ratio,:,:]
    return X_train, X_test, Y_train, Y_test


X_train,X_val,Y_train,Y_val=data_split(train_combined,groundtruth_patch,3)
print (X_val.shape)
print (X_train.shape)


## Code for focal loss ##

import tensorflow as tf
def focal_loss(gamma=2., alpha=.25):
    def focal_loss_fixed(y_true, y_pred):
        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))
        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))
        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0)) 
    return focal_loss_fixed



## Model Executation
input_img = Input((im_width, im_height,1), name='img')
model = get_segnet(input_img, n_filters=32, dropout=0.05, batchnorm=True)                             ### Change to U-net/Seg-Net accordingly. In case of U-Net, n_filters=16. For segnet n_filters=32
model.compile(optimizer=Adam(),loss=[focal_loss(alpha=.25, gamma=2)], metrics=["accuracy"])
model.summary()


## Saving the best predictions

callbacks = [
    EarlyStopping(patience=10, verbose=1),
    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),
             ModelCheckpoint('model-tgs-salt.h5', verbose=1, save_best_only=True, save_weights_only=True)      ## Best model gets saved as 'model-tgs-salt.h5'
]


results = model.fit(X_train, Y_train, batch_size=8, epochs=10,callbacks=callbacks,
                    validation_data=(X_val, Y_val))



#References:

#[1] Ronneberger, O., Fischer, P. and Brox, T., 2015, October. U-net: Convolutional networks for biomedical image segmentation. In ​International Conference on Medical image computing and computer-assisted intervention​ (pp. 234-241). Springer, Cham
#[2] Badrinarayanan, Vijay, Alex Kendall, and Roberto Cipolla. "Segnet: A deep convolutional encoder-decoder architecture for image segmentation." ​IEEE transactions on pattern analysis and machine intelligence 39, no. 12 (2017): 2481-2495
#[3] github.com/orobix/retina-unet
#[4] towardsdatascience.com/understanding-semantic-segmentation-with-unet-6be4f42d4b47
#[5] Lin, T.Y., Goyal, P., Girshick, R., He, K. and Dollár, P., 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision​ (pp. 2980-2988).
#[6] www.isi.uu.nl/Research/Databases/DRIVE/     



